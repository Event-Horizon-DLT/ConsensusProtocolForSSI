# Introduction (SECTION I: SUMMARY)

Personal data aggregation and mining is a Trillion dollar industry, proving the value of analysing human behaviour through digital evidence.
As often, a phenomena of this amplitude inherently carries a part of light and a part of shadow: while it provides a formidable basis for scientific investigation, in psychology, economics, sociology, etc. it also offers mean for manipulation, disinformation and allows to violate privacy and limit freedom.
What rules and boundaries can keep shadows at bay and allow the light to expand? How can we avoid human rights alienation but allow progress to follow it course?
The answer is primarily political and regulatory: a balance has to be found by the democratic institutions, to inspire fair and clear regulatory guidelines, control their implementation and enforce their observance. The Cambridge Analytica process epitomised the lack of legal protection against personal data predators and manipulators. Since then, most democratic systems try to fill the gap: the European Union has reinforced its regulation through the adoption for the GDPR %%acronymreference in 2018, and various other countries have taken similar measures %%examplereference.
Beside formal democratic representation, several movements have emerged from all over the world. They campaign and actively push for further action. At the border between Politics and Technology, these initiatives explore means of protection, independent from central regulation and public institutions: Self Sovereign Identity is at the heart of most of these initiatives. This “Internet of Identity” (cf. Phil Windley, 2016) aims at offering to each user a decentralised, independent and trustable identity layer. 
Since internet was initially built without such a layer, we all had to create different users, and track their related passwords to access all the different applications, services and networks we wanted or needed. Naturally, when several platforms proposed to leverage on their own identity layer to ease our digital activity, we started to “login using (…)”, thus transferring to these platforms and networks full access to our digital personality. The companies owning these platforms started to concentrate an extraordinary amount of personal data, and developed mobile technology, messaging, big data and cloud computing to gather, mine and market this information, creating a trillion dollar industry.

## SSI
Born as an initial effort to solve the problem of internet users multiple ids, Self Sovereign Identity (SSI) now aims at providing an alternative to this concentration of power and information. The major technology breakthrough that makes this vision possible is the Decentralised Ledger Technology: a framework that uses consensus amongst pairs to eliminate the need for any central authority (and single point of failure). To be viable, SSI requires an environment of “diffuse trust, not belonging to or controlled by any single organisation or even a small group of organisations” (cf. The Sovrin Foundation, 2017).
But how can Self Sovereign identity work? Christopher Allen provides a formal definition (cf. Christopher Allen, 2016), that can be summarized as the conjunction of 3 key conditions: Security, Controllability and Portability. Allen breaks down each one of these 3 conditions into more specific attributes, but the essence is this: Provide users %%simpledefinitionofSSI
%%summarisehowitcomestogetherW3CSovrinToCreateAStandar->DID

### SSI and DLT
The role of Decentralised Ledger Technology is crucial: it provides a transparent and secure environment to all identification activity (login, localisation, activity tracking, …), it effectively enforces a trustable layer without any central body to control it: control is mutual, and transparent: anyone can test and verify that a specific identifier, or related attribute is legitimate, active and uncompromised.
In this framework, roles get re-distributed : entities currently used as source of Digital Identity are still crucial, but they act as verificators. Data Subject Identity and personal data can no longer be aggregated without their explicit consent, and without granting them full access to whatever information is stored about them. 
How far are we today from turning this vision into reality?
The first challenge for any decentralised network is consensus. Consensus can only emerge if the network nodes speak the same language, and follow the same protocol: precisely, standardisation has come a long way: the W3C committee for Self Sovereign Identity plan to complete their undergoing specification for DID standardisation in 2021. This standard will ensure interoperability, but most of the work is already done, and enough consensus has been reached for multiple platforms to already develop solutions that implement SSI. An entire ecosystem is about to emerge, allowing end users to navigate the web with reduced concern for their privacy. 
The second challenge, after standardization, is mass adoption: how can we move from early prototype to lead users activism and finally mass adoption? 
Two main ingredients are needed for this: 
-	first the solution needs to be ready, i.e. it has to be: 
o	reliable
o	useable 
o	scalable
-	second, the moment has to happen: external events will create a favourable context, and finally some trigger will start the mechanic of mass adoption (%%REFtipping point). 

Whatever the events, and the context, mass adoption will only happen if the first condition is already fulfilled, else it will just be another lost opportunity.
Since the second condition is contingent, let us focus on the first one: how to build a relevant, usable and scalable SSI framework?
This question itself can be broken down into several key aspects:
-	Where should decentralised personal data be stored? 
-	How can th%%multiplequestions

### The Data custody problem
Imagine a vault in which your passport, driving license, medical data and other sensitive private information is stored. Imagine that each one of the respective issuers sent you a key for each one of these documents under strict confidentiality. Whenever needed, you can now authenticate using this key and the issuing entity (or any other patented validator) will confirm that, respectively: you own a valid driver license, you are over 21, etc. Now, you can start navigating on websites that have age restrictions, rent a car online without using any personal credential (email ID, social network ID…): all it takes is a network between the service you want to access, the ID verificator and yourself, to create a sufficient level of trust. 
To get back to our metaphor, you do not show your passport anymore, it stays in the vault, you just prove whatever needs to be proven, when it’s required. Now, your different IDs allow you to effectively perform these activities, but make it much harder to tie them to your identity for any third party, since all the outside world can see are keys and IDs, with no relation between them.
Now for this to work, there is still a need for a Vault – a safe place where your complete set of personal data will be kept, your private keys and related IDs are effectively linked together, allowing you to use them – Where could that be? 
Inherited from Cryptocurrencies, the concept of Wallet seems to be the natural solution: wallets can either be software or a combination of soft and hardware, to encrypt and hold sensitive data (in the case of cryptocurrencies, this is where your coins will be stored). If you lose your wallet, you lose whatever is inside. If you forget your credentials and keys, you can no longer access your wallet either. Finally, if your wallet is compromised you can lose its content for good. 
Examples of such wallets already exist, some of them being apps that can store your IDs and Keys. Is this really a reliable, usable and scalable solution? Isn’t this putting the data subject in a difficult position: what if this wallet gets lost? Is there a backup on the cloud and if so, how is this backup stored, monitored, protected – aren’t we creating again a single point of vulnerability, where personal data can again be hacked or even simply sold without end user being aware?
Why not use instead, the robustness, safety and resilience of a Decentralised Ledger to host and protect this vital information?
After all, cryptocurrency Wallets addresses are personal data (according to the European regulator %%%refreenceEUregulator Wallet addresses  = personal data) , and they are stored in several cryptocurrency ledgers, starting with BitCoin. Yet there is reluctance to extend the coverage of this personal data storage on a Decentralised Ledger. This reluctance is explained by the following objections:
1.	Encrypted personal data would be exposed to anyone having access to the ledger – and the most robust encryption algorithms can be cracked one day in the future
2.	Since Personal Data are submitted to the right of modification and revocation by the data subject: how can a –by definition immutable – Ledger effectively keep up?
3.	Identity is dynamic, personal data change with our life, again a challenge for an immutable ledger
4.	Since the decentralised ledger has to be replicated on every single node of the network, how could it support millions (if not billions) of personal data records without becoming rapidly oversized, threatening the solution scalability?
These 4 objections are legitimate, but does it mean that hosting personal data in a DLT is strictly incompatible with a reliable and efficient SSI solution? Let us lift for a moment this incompatibility hypothesis, revert it and ask the following question: What are the requirements for a DLT to be the ultimate repository of personal data in an SSI framework?

### Solving the Data Custody problem
First of all, lifting the incompatibility hypothesis allows to draw the following proposal for a viable SSI Framework:
-	source : data issuer (e.g. government body, bank, utilities, health services…)
-	owner: data subject ( focusing on individuals here – though legal entities should also be treated as data subjects, to a certain extend)
-	solicitor: individual or legal entity needing to verify that the data or proof of data provided by a data subject is valid (e.g. flat owner wanting to rent to someone, checking his credit worthiness) 
-	validator: trusted intermediary running validation nodes, receiving requests, controlling the data subject approval for answering these requests, contacting the data issuers to control the data subject stattus and finally issuing the proof initially requested. The need for Decentralised Ledger Technology %%refsovrinandGoodell for this layer is clearly established: all these transactions will have to be audited, traced and accessible to regulators and data subjects on a real time basis.
-	data custodian: in this proposition, since the personal data is hosted in a Decentralised Ledger, this function is vital. Custodians are the safekeepers of individuals encrypted personal data. They will receive/process/accept or reject requests of write and read data. In some cases they will receive requests for proof by the validators themselves, if the issuers decide to delegate this verification function. Data custodian are the guaranty that, if end users personal data containers (wallets) are compromised, lost or locked, they have the latest status. This function can only be run by a number of independent, trustable non profit entities, linked together in one or several interconnected decentralised networks. They are the cornerstone of (lack of) trust in the system. Whether Validators can also be Custodians, and whether one single DLT should host both the validation and data custody layer remains to be carefully analysed. In this analysis, we will treat them as 2 systematically separated functions. 

This framework being defined, we want to focus on the viability of a DLT for data custody..
There are different kinds of Decentralised Ledger Technologies (DLTs), some of them more fit for the purpose of Data protection in general, and SSI in particular.
A DLT is made of different components, including encryption, consensus protocol, ledger, network, etc. but what will determine a DLT relevance for SSI are the consensus protocol, the network governance structure and the Ledger itself. 
Following the 3 key properties for mass adoption (reliability, usability, scalability), together with the SSI inherent requirements, we can describe a model for such a Decentralised Technology.

## Reliable SSI: Governed, permissioned network
To host personal data, a Decentralised Ledger needs to run on fully trustable nodes. Therefore it has to be a permissioned network, where nodes can only join the network if its governance structure approves. And this governance structure has to follow clear and verifiable rules, for authorising new joiners.
A number of DLT believers disapprove the notion of submitting network access to permisioning: unpermissioned networks are, in their view, the only guaranty for enduring fairness and democratic processes. Experience shows however that unpermissioned networks are subject to concentration, where a high proportion of nodes are effectively run by a limited number of actors. In fact, recent academic research demonstrates that this concentration phenomenon will happen systematically if an unpermissioned network attracts sufficient value %%REFUnpermissionedconcentration.
The network Governance is another key condition for the solution reliability. It sets the rules and processes by which nodes are created, entities authorised to join the custodian layer, etc. The history of DLT is full of attempts to embed the network governance itself in the ledger consensus protocol itself  #REFdltgovernancemechanisms. These can be justified in specific context, and whenever the consensus protocol allows it: Proof Of Stake is an example of governance where the fate of the whole is decided by the majority of stakes held by the different network members. For the protection of data sovereignty however, social sciences and human experience  show that the democratic system is the only proven solution, in spite of its many deficiencies (%%REFChurchilldemocracyisthelessofallsystems)… (constitution, regulator involvement, etc.)
Last but not least, a reliable Data Custodian Layer has to face objection #1: the ledger will effectively centralise billions of individual personal data, encrypted and protected, but potentially vulnerable to the evolution of computer science. A solid argument, since no encryption algorithms has been demonstrated as theorically unbreakable %%REFencryptionalgo. In other words: it is today practically impossible to break encryption algorithms because of the prohibitive computational power and time to resolution it would require. But as technology evolves, and the stakes are big enough (e.g. double spend BITCOINs or access billions of individual personal data), the investment might soon become worth it.  
However this argument does not take into account the dynamic of encryption science, rather its current static state of the art: while the progress of computer science empowers deciphering, encryption is also evolving, and several research groups are actively working on quantic computing resilient encryption. SSI DLTs need to be designed from inception as forward compatible with alternative encryption protocols. The encryption/deciphering functions need to be modular enough to evolve when necessary. The consensus algorithm itself has to be abstracted from the encryption layer – which excludes several existing consensus protocols. 
The second aspect to consider is whether this Decentralised Ledger improves or degrades the situation: what is the alternative? Every individual holding their personal data in their own SSI Wallet, installed on their mobile devices, with a credential recovery procedure and perhaps a hard drive backup (a contingent notion, as every individual would have to devise their own backup mechanism). These personal data have been issued by various entities, each one asking for a number of information before releasing them. Recover from a loss or break would be at least tedious, at worst damageable. And since this is about personal data, which follow us our entire life, the solution has to account for incidents to necessarily happen, probably several times for every individual. The whole system is only reliable with a practical recovery process. Most likely, data subjects will be tempted to backup information on the cloud – which leads us to square one: without a reliable, resilient, independent personal data storage utility, these data will be as vulnerable as they are today. To the second question, we are therefore tempted to answer: a Decentralised Ledger for personal Data improves the viability of an SSI solution.
Thirdly, the consequences of a safety breach into the personal Data Custody can be mitigated. A key element of personal data protection is to make aggregation difficult. The implementation of SSI through Decentralised IDs creates multiple identities, with no link between them, other than the personal data owner himself, and the wallet where he decides to centralise them. Hosting personal data on a Ledger does not mean binding the different IDs held by a same individual together: each identity will be accessed with its own set of keys, independent identifiers, and the aggregation layer does not need to be visible from the Personal Data Ledger. In other words, if the DLT only held the pieces of billions of different jigsaws, but no mean to tie them together, breaking into the ledger will only provide access to anonymous random data. Only the conjunction of controlling the personal data container (wallet) and breaking the DLT will provide valuable information. But in any case, controlling the personal data wallet already provides all needed information to a potential intruder. Here again, hosting the data on a secured decentralised ledger does not worsen the situation, provided the Ledger Data structure fulfil the atomisation condition: one identity for each piece of personal data stored, no mean to bind them together from the Ledger. The recovery of atomised IDs can be organised in a both practical and safe procedure, similar to the recovery of credentials, and could be completed in less than a day. 

Usable SSI needs a Pragmatic Byzantine Fault Tolerant consensus protocol 
Many technologies did not live up to their potential because they never got adopted by a sufficient proportion the population, never reached this critical mass, or “tipping point”, from which massive adoption is triggered. 
There can be multiple reasons why a performing and relevant technology is not adopted: it has to comply with regulation, it has to be accessible (technically, economically, geographically…), and, in addition of these, it has to be trusted by potential adopters, especially when dealing with sensitive subjects such as safety, health, finance or freedom. 
The meaning we give to “usability” precisely refers to this property: it means the capacity of a given technology to be massively adopted by its natural end users population. 
What makes a data custodian technology usable, and improves its potential of adoption? When considering sensitive Data custody, the first thing that comes to mind is: trust. How can I trust a third party to hold my sensitive data, and not lose it, not disappear, not being breached, not use it for profit, or worst, control? We already discussed the solution reliability, which is necessary to gain potential users trust, and argued that a decentralised, permissioned, transparently governed decentralised network respecting data atomisation could be considered reliable. Reliability is necessary yet not sufficient to gain trust. For large portions of the population to entrust their personal data to a consortium of data custodians, their technical solution has to also be resilient, and transparent. 
Resilience first, in this context, is the system capacity to quickly recover from extreme events, such as disaster, major technical failures and hostile intrusion. The main property of DLT, the reason of its very inception actually, is resilience: the ledger is synchronised between independent, physically separated nodes, so disasters or failures don’t event interrupt the service. Just to clarify: resilience or even high availability in the face of disasters and failures can be partially addressed without DLT: replication servers and file mirroring are typical tools for this purpose. The main difference, however is, that replicating/mirroring the action of 1 single process does not remove the Single point of failure risk. Should something go wrong with the main process, and be stored in the main Ledger, it will propagate to all replicas. What Decentralised Ledger Technology brings, on top of this physical replication, is a consensus layer amongst peers: data and events have to be processed in parallel by the nodes, and they need to reach an agreement to update the Ledger. Does this completely eliminate the risk of failure? Not entirely: should a high enough number of nodes fail or be compromised, the entire network will be at risk– but it makes this event very unlikely.
The way nodes reach agreement is therefore key to inspire trust: the logic of consensus, called “consensus algorithm”, determines how the network runs, its scalability, and ultimately most of its strengths and weaknesses. 
%%SHORTTRANSITION
The most adequate consensus algorithm for a usable Data Custodian service has to be a form of Practical Byzantine Fault tolerant voting. Byzantine Fault Tolerant protocols are based on the original Byzantine General problem, formalised in 1982 by (%%REF): the solution to this problem is to organise consensus in such a way that the Network will work without impediment as long as at least 2/3 of the nodes are non faulty and non compromised. 
This type of protocol has a number of advantages over their alternatives: Proof Of Work (PoW), Proof Of Stakes (PoS), etc. 
Firstly, since the network will be permissioned, the main advantage of Proof Of Work - its simplicity, and efficiency to reach agreement amongst a very large number of unknown nodes- is less relevant. Its drawbacks, however, are eliminatory: 

### Pragmatic Byzantine Fault Tolerant vs. Proof of Work
1.	PoW is not easily scalable: the larger the network the harder the mathematical Proof – and, more importantly the size of each block is limited. Therefore, when the number of events to be processed increases, bottlenecks appear and the time to process deteriorates in a non-linear fashion. As the size of the network (number of nodes) grows, Byzantine Fault Tolerant protocols also have to solve for the increased time needed to broadcasts events and votes across larger networks. However, since we work under the hypothesis of a permissioned, governed network, the number of nodes can be optimised by the governing structure. On the other hand, a BFT network can be fully scalable to the size and number of data events processed by the nodes themselves: it is not tight to a block-chain type of Ledger. 
2.	Second issue with Blockchain as a Ledger format: it is virtually immutable, while a regulatory compliant Data Custody Ledger has to allow data subject to call for modification or to revoke their data. The Ledger structure and protocol on a Data Custody Byzantine Fault Tolerant network will work differently: at a given point in time, a watermark is created - the network nodes agree on the corresponding state – expunging data that has to be eliminated or modified, signing and certifying these actions before storing them for transparency purpose. Not entering into the details here, as this type of event is quite complex to implement, but it effectively creates a new foundation block for the Ledger, from which new events will be aggregated. 
3.	Third point, also important for trust: PoW is less resilient than BFT: A 51% attack can take control of the network and rewrite the ledger entirely. In BFT, only 66%+1 of the nodes can effectively modify the Ledger. In a permissioned network run by reliable institutions, reaching that quorum becomes extremely unlikely for a hostile, coordinated intruder.
4.	Finally, PoW is an ecological disaster. How could trustable organisations running the network use a consensus algorithm that consumes as much electrical power as the entire Czeck Republic, just to run Bitcoin? How could potential end users embrace it, when we all grow increasingly concerned about human activity impact on the planet and our health? A BFT network reaches consensus through voting, not by repeatedly trying to solve an energy-thirsty mathematical equation. There is no competition between the nodes to collect a fee. Its carbon footprint is therefore much lower, and should be stay so as long as a robust and lean engineering drives the solutions design. 

##Pragmatic Byzantine Fault Tolerant vs. Proof of Stake
1.	Proof of Stake is a voting protocol where voters are not peers: each vote will be weighted by the voter stake in the network. The stake is generally a monetary collateral (FIAT or Crypto currency). There are multiple use cases in which the notion of Stake can make sense, especially – again – in un-permissioned networks. In the case of Data custody, permissioned and governed networks, adding stake to weight the votes does not bring any additional value to the consensus, and adds unnecessary complexity.
2.	Proof of Stake effectiveness entirely depends on the stakeholders responsibility: the penalty incurred by any node acting against the rules is the loss of its collateral, used to weight the vote. However the dissuasive impact of losing collateral does not work for compromised nodes, whereby the real Stakeholder has lost control. This makes this protocol much less reliable for sensitive data protection, where the level of threat is maximal, and only systems built for this maximal threat level are standing a chance.

Other consensus protocols, less relevant to the context, are not mentioned here. 
BFT as a consensus protocol is clearly the most adapted for secured, decentralised and transparent custody of personal data. 
Looking back at the 4 arguments against personal data custody, we briefly mentioned how a properly designed BFT based DLT can address arguments #2 and #3: data subjects must be able to update their personal data, and even revoke them. This feature is complex to design and implement in a DLT, but crucial for the usability of a decentralised personal data custody service. The purpose of this summary is not to develop our solution to this problem, but more details will be published shortly. 

### Scalable SSI: efficiently process billions of data sets
The third property of a mass adoption-ready technology is its scalability. This also corresponds to argument #4: How can a DLT store billions of data sets, process hundreds of millions of daily concurrent requests without collapsing? 

#### Firstly: storage capacity. 
One of the challenges for a typical blockchain scalability is its recursive structure: to compute the balance of an account, you need to rebuild its entire transaction history. Where you could store one single record, you potentially need hundreds of them, the whole history. The advantage of this structure however is: any third party accessing the Blockchain can control that the balance is correct, and what this balance is. This transparency is very useful to establish trust between counterparties. It also makes the Blockchain data very difficult to tamper with: to change today’s picture, it will be necessary to move the whole history of transactions and events that led to it. This works particularly well on cumulative data objects: account balances, inventories, performance time series, etc. Personal data however is not necessarily cumulative: an address, a date of birth, a legal status, etc. have little to do with a chain of events. Yet this data is extremely sensitive and needs to be efficiently protected from unwanted access and manipulation. It is therefore necessary to provide a state of the data, that can still be verified and rebuilt from a previous reference point, by all the nodes of the networks, going through the needed cryptographic controls (hashing, cyphering, keys validation, etc.). This reference point – genesis block in a traditional blockchain – does not need to be however the very first set of data ever issued. It should be instead, the last voted reference point, for all the nodes, according to a very specific watermark creation procedure. This reduces the volume of data to be process daily request while maintaining a high level of protection. Furthermore, the events related to the establishment of consensus (ballot history) can be kept for audit and rebuild structure, without impacting the volume of data involved in personal data transactions themselves. Through these mechanisms, and an optimised physical ledger structure, billions of records can be maintained online, accessed and updated without suffering scalability issues. 

#### LAST BIT IS ABOUT NETWORK SCALABILITY (REACH FAST CONSENSUS IN SPITE OF INCREASING NODES NUMBER – FAIRLY QUICK AND EASY SINCE NETWORK HAS LIMITED NUMBER OF NODES)

